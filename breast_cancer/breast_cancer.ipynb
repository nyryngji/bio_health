{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# 암이 전이 안 됨 : 514, 암이 전이가 됨 : 486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['img_path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path in train['img_path']: # test도 똑같이 수행\n",
    "    img_name = img_path.split('/')[-1] # 이미지 이름\n",
    "    new_img = []\n",
    "    new_img2 = []\n",
    "\n",
    "    # 가로 방향으로 불필요한 흰색 배경 제거\n",
    "\n",
    "    img = cv2.imread(img_path)[200:-200,200:-200,] # 이미지에 줄 같은 게 있어서 이를 제거하기 위해 200px씩 상하좌우로 제거\n",
    "    img2 = cv2.cvtColor(img[:], cv2.COLOR_BGR2GRAY) # 이미지를 흑백으로 전환 \n",
    "    ret, binary = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY) # 이미지 이진화\n",
    "    binary = cv2.cvtColor(binary, cv2.COLOR_BGR2GRAY) # (?,?,3) -> (?,?)\n",
    "    rows_to_delete = [i for i in range(len(binary)) if set(binary[i]) == {255}] \n",
    "    # 만약 이진화 이미지에 임파선이 없고 모두 흰 배경인 index\n",
    "\n",
    "    for i in range(3):\n",
    "        new_img.append(np.delete(img[:,:,i], rows_to_delete, axis=0)) # RGB 채널별로 흰 배경을 제거\n",
    "        # 컬러 이미지를 유지하기 위해서\n",
    "\n",
    "    new_img = np.array(new_img).T.swapaxes(0, 1) # -> (3, 2603, 5741) -> (5741,2603,3) -> (2603, 5741, 3)\n",
    "\n",
    "\n",
    "    # 세로 방향으로 불필요한 흰색 배경 제거\n",
    "\n",
    "    img = new_img\n",
    "    img2 = cv2.cvtColor(img[:], cv2.COLOR_BGR2GRAY) # 흑백 전환\n",
    "    ret, binary = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY) # 이미지 이진화 \n",
    "    binary = cv2.cvtColor(binary, cv2.COLOR_BGR2GRAY) # 이진화된 이미지 흑백 전환\n",
    "\n",
    "    rows_to_delete = [i for i in range(len(binary.T)) if set(binary.T[i]) == {255}] # 흰색만 있는 영역의 index\n",
    "    for i in range(3):\n",
    "        new_img2.append(np.delete(img[:,:,i].T, rows_to_delete, axis=0)) # RGB 채널별로 흰 배경 제거\n",
    "\n",
    "    new_img2 = np.array(new_img2).T\n",
    "    new_img2 = cv2.resize(new_img2,(1000,1000)) # 이미지가 cnn이 학습할 수 있게 (1000,1000,3)으로 사이즈 조절\n",
    "\n",
    "    denoised_image = cv2.fastNlMeansDenoisingColored(new_img2, None, 10, 10, 7, 21) \n",
    "    # 이미지에 존재하는 희미한 점 등을 제거\n",
    "\n",
    "    res = Image.fromarray(denoised_image) # 넘파이 배열을 이미지로 변환 \n",
    "    res.save(f'./test_img_color/{img_name}','PNG') # png파일로 해당 이미지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    for img_path in list(train[:][train['N_category'] == i]['img_path']):\n",
    "        x = img_path.split('/')\n",
    "        shutil.copy(img_path, f'.\\\\trainset\\\\{str(i)}\\\\' + x[-1])\n",
    "\n",
    "# train 데이터셋을 만들기 위해 trainset에 클래스별로 폴더 생성 후 이미지를 복제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  'D:\\cancer_dacon\\\\trainset',\n",
    "  validation_split=0.1,\n",
    "  subset=\"training\",\n",
    "  seed=42,\n",
    "  image_size=(1000,1000))\n",
    "\n",
    "# 기존 폴더에서 train데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(1000, 1000, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Flatten()\n",
    "])\n",
    "\n",
    "# CNN 모델 생성, 1차원 배열에서 최종적으로 이미지의 특성을 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['img_path'] = train['img_path'].str.replace('train_imgs','train_img_color')\n",
    "test['img_path'] = test['img_path'].str.replace('test_imgs','test_img_color')\n",
    "\n",
    "# 새롭게 전처리한 이미지가 있는 폴더를 대상으로 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_feature = []\n",
    "\n",
    "for img_name in train['img_path']:\n",
    "    img = cv2.imread(img_name)\n",
    "    img = np.expand_dims(img, axis=0)  # 배치 차원 추가 (1, 500, 500, 3)\n",
    "\n",
    "    f = model.predict(img)\n",
    "    train_img_feature.append(f[0]) # model이 반환한 이미지의 특성을 리스트에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_feature = []\n",
    "\n",
    "for img_name in test['img_path']:\n",
    "    img = cv2.imread(img_name)\n",
    "    img = np.expand_dims(img, axis=0)  # 배치 차원 추가 (1, 500, 500, 3)\n",
    "\n",
    "    f = model.predict(img)\n",
    "    test_img_feature.append(f[0]) # model이 반환한 이미지의 특성을 리스트에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    if train[col].isnull().sum() >= 700:\n",
    "        train = train.drop(col,axis=1)\n",
    "        test = test.drop(col,axis=1)\n",
    "\n",
    "# 결측치가 70%를 넘어가면 해당 컬럼 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    if train[col].isnull().sum() >= 1 :\n",
    "        train[col] = train[col].fillna(train[col].mode()[0])\n",
    "        test[col] = test[col].fillna(test[col].mode()[0])\n",
    "\n",
    "# 결측치의 경우 각 컬럼의 최빈값으로 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['histology_grade'] = train[['HG_score_1','HG_score_2','HG_score_3']].sum(axis=1)\n",
    "test['histology_grade'] = test[['HG_score_1','HG_score_2','HG_score_3']].sum(axis=1)\n",
    "\n",
    "train['histology_grade'] = np.where(train['histology_grade']<=5,1,\n",
    "                           np.where(train['histology_grade']<=7,2,3))\n",
    "test['histology_grade'] = np.where(test['histology_grade']<=5,1,\n",
    "                           np.where(test['histology_grade']<=7,2,3))\n",
    "\n",
    "# 세 점수를 더하면 조직학적으로 악성정도를 등급(1,2,3)을 매길 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['surgery_year'] = pd.to_datetime(train['수술연월일']).dt.year.astype('int64')\n",
    "train['surgery_month'] = pd.to_datetime(train['수술연월일']).dt.month.astype('int64')\n",
    "train['surgery_date'] = pd.to_datetime(train['수술연월일']).dt.day.astype('int64')\n",
    "\n",
    "test['surgery_year'] = pd.to_datetime(test['수술연월일']).dt.year.astype('int64')\n",
    "test['surgery_month'] = pd.to_datetime(test['수술연월일']).dt.month.astype('int64')\n",
    "test['surgery_date'] = pd.to_datetime(test['수술연월일']).dt.day.astype('int64')\n",
    "\n",
    "# 수술 연월일을 각각 분리하여 새로운 컬럼 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_bad_col = ['NG', 'HG', 'HG_score_1', 'HG_score_2', 'HG_score_3',\n",
    "                'DCIS_or_LCIS_여부', 'T_category', 'ER', 'ER_Allred_score', 'PR',\n",
    "                'PR_Allred_score', 'KI-67_LI_percent', 'HER2', 'HER2_IHC']\n",
    "\n",
    "for col in down_bad_col:\n",
    "    train[col] = np.where(train[col]==4,0,train[col])\n",
    "    test[col] = np.where(test[col]==4,0,test[col])\n",
    "\n",
    "# 해당 컬럼에서 4는 증상이 미미하여 거의 없다고 판별하기에 0과 동급으로 치환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['serious'] = train[down_bad_col].sum(axis=1)\n",
    "test['serious'] = test[down_bad_col].sum(axis=1)\n",
    "\n",
    "# 숫자가 커질수록 안 좋은 컬럼들의 합계를 구함\n",
    "# 숫자가 클수록 예측이 1에 가까울 것이라고 가정하여 해당 전처리 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['ID','수술연월일','mask_path','img_path'],axis=1)\n",
    "test = test.drop(['ID','수술연월일','img_path'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['img_feature'] = train_img_feature\n",
    "test['img_feature'] = test_img_feature\n",
    "\n",
    "# 위에서 구한 이미지의 특성을 데이터셋에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 리스트 데이터를 개별 컬럼으로 변환\n",
    "img_features_train = np.stack(train['img_feature'].values)\n",
    "img_features_test = np.stack(test['img_feature'].values)\n",
    "\n",
    "# PCA 차원 축소 -> 128개는 너무 많기 때문\n",
    "pca = PCA(n_components=5) # 5개로 0.9904% 설명 가능\n",
    "train_pca = pca.fit_transform(img_features_train)\n",
    "test_pca = pca.transform(img_features_test)\n",
    "\n",
    "# PCA 결과를 데이터프레임으로 변환\n",
    "train_pca_df = pd.DataFrame(train_pca, columns=[f'pca_feature_{i}' for i in range(5)])\n",
    "test_pca_df = pd.DataFrame(test_pca, columns=[f'pca_feature_{i}' for i in range(5)])\n",
    "\n",
    "# 기존 데이터프레임에서 img_feature 제거 후 병합\n",
    "train = train.drop(columns=['img_feature'])\n",
    "train = pd.concat([train, train_pca_df], axis=1)\n",
    "\n",
    "test = test.drop(columns=['img_feature'])\n",
    "test = pd.concat([test, test_pca_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = train.drop('N_category',axis=1)\n",
    "ytrain = train['N_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain2,xval,ytrain2,yval = train_test_split(xtrain,ytrain,random_state=42, test_size=0.2)\n",
    "xtrain2.shape, xval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier(random_state=42)\n",
    "model.fit(xtrain2,ytrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model.predict(xtrain2)\n",
    "pred2 = model.predict(xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(accuracy_score(ytrain2,pred1))\n",
    "print(precision_score(ytrain2,pred1))\n",
    "print(recall_score(ytrain2,pred1))\n",
    "print(f1_score(ytrain2,pred1))\n",
    "print()\n",
    "print(accuracy_score(yval,pred2))\n",
    "print(precision_score(yval,pred2))\n",
    "print(recall_score(yval,pred2))\n",
    "print(f1_score(yval,pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['N_category'] = pr\n",
    "submit.to_csv('./submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
